{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82e48af8-affe-4717-b737-3197de8b5b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5d30ea8-0482-4460-be4b-ad9bd85eec02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   id keyword location                                               text  \\\n",
       " 0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       " 1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       " 2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       " 3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       " 4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       " \n",
       "    target  \n",
       " 0       1  \n",
       " 1       1  \n",
       " 2       1  \n",
       " 3       1  \n",
       " 4       1  ,\n",
       "    id keyword location                                               text\n",
       " 0   0     NaN      NaN                 Just happened a terrible car crash\n",
       " 1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       " 2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       " 3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       " 4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan,\n",
       "    id  target\n",
       " 0   0       0\n",
       " 1   2       0\n",
       " 2   3       0\n",
       " 3   9       0\n",
       " 4  11       0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(r\"C:\\Users\\Nourhan Yehia\\Desktop\\Jupyter\\nlp project\\nlp-getting-started\\train.csv\")\n",
    "test_df  = pd.read_csv(r\"C:\\Users\\Nourhan Yehia\\Desktop\\Jupyter\\nlp project\\nlp-getting-started\\test.csv\")\n",
    "sub_df   = pd.read_csv(r\"C:\\Users\\Nourhan Yehia\\Desktop\\Jupyter\\nlp project\\nlp-getting-started\\sample_submission.csv\")\n",
    "\n",
    "train_df.head(), test_df.head(), sub_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b06780a9-8272-4e77-9dda-ddfea2a4cdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3263 entries, 0 to 3262\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        3263 non-null   int64 \n",
      " 1   keyword   3237 non-null   object\n",
      " 2   location  2158 non-null   object\n",
      " 3   text      3263 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 102.1+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3263 entries, 0 to 3262\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   id      3263 non-null   int64\n",
      " 1   target  3263 non-null   int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 51.1 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    7613.000000\n",
       "mean      101.037436\n",
       "std        33.781325\n",
       "min         7.000000\n",
       "25%        78.000000\n",
       "50%       107.000000\n",
       "75%       133.000000\n",
       "max       157.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.info()\n",
    "test_df.info()\n",
    "sub_df.info()\n",
    "\n",
    "train_df.isnull().sum(), test_df.isnull().sum()\n",
    "train_df[\"target\"].value_counts(), train_df[\"target\"].value_counts(normalize=True)\n",
    "train_df[\"text\"].str.len().describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3efc1d65-6866-4aac-b19d-2c0f9880c10f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(keyword     0\n",
       " location    0\n",
       " text        0\n",
       " dtype: int64,\n",
       " keyword     0\n",
       " location    0\n",
       " text        0\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_cols = [\"keyword\", \"location\", \"text\"]\n",
    "\n",
    "train_df[text_cols] = train_df[text_cols].fillna(\"missing\")\n",
    "test_df[text_cols]  = test_df[text_cols].fillna(\"missing\")\n",
    "\n",
    "train_df[text_cols].isnull().sum(), test_df[text_cols].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d851e868-8761-4e9f-b3c0-a37533f85e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62eee31a-7222-430c-9abd-02f3af3fc934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_lemmatize(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)  # remove URLs\n",
    "    text = re.sub(r\"<.*?>\", \" \", text)            # remove HTML tags\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)         # remove special chars/numbers\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    tokens = word_tokenize(text)                  # tokenize\n",
    "    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]\n",
    "    tokens = [lemm.lemmatize(t) for t in tokens]  # lemmatize\n",
    "\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6219df65-77fd-44a8-a06e-3e4dc92e4985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>deed reason earthquake may allah forgive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>forest fire near ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>people receive wildfire evacuation order calif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword location                                               text\n",
       "0  missing  missing           deed reason earthquake may allah forgive\n",
       "1  missing  missing                 forest fire near ronge sask canada\n",
       "2  missing  missing  resident asked shelter place notified officer ...\n",
       "3  missing  missing  people receive wildfire evacuation order calif...\n",
       "4  missing  missing  got sent photo ruby alaska smoke wildfire pour..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in text_cols:\n",
    "    train_df[col] = train_df[col].apply(clean_and_lemmatize)\n",
    "    test_df[col]  = test_df[col].apply(clean_and_lemmatize)\n",
    "\n",
    "train_df[text_cols].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff4ffe7b-a287-4b60-8e1b-3865baac5799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>deed reason earthquake may allah forgive</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>forest fire near ronge sask canada</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>people receive wildfire evacuation order calif...</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  tweet_len\n",
       "0           deed reason earthquake may allah forgive         40\n",
       "1                 forest fire near ronge sask canada         34\n",
       "2  resident asked shelter place notified officer ...         85\n",
       "3  people receive wildfire evacuation order calif...         51\n",
       "4  got sent photo ruby alaska smoke wildfire pour...         54"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"tweet_len\"] = train_df[\"text\"].str.len()\n",
    "test_df[\"tweet_len\"]  = test_df[\"text\"].str.len()\n",
    "\n",
    "train_df[[\"text\",\"tweet_len\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "575d7d76-165d-467a-b910-5ad524ea9dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6090, 4), (1523, 4))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = train_df[[\"keyword\", \"location\", \"text\", \"tweet_len\"]]\n",
    "y = train_df[\"target\"]\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "X_train.shape, X_valid.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "577b88f9-08f2-43c7-a4e1-c58abf4b4221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "text_features = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"keyword_tfidf\",  TfidfVectorizer(ngram_range=(1, 2)), \"keyword\"),\n",
    "        (\"location_tfidf\", TfidfVectorizer(ngram_range=(1, 2)), \"location\"),\n",
    "        (\"text_tfidf\",     TfidfVectorizer(ngram_range=(1, 2)), \"text\"),\n",
    "        (\"tweet_len\",      \"passthrough\", [\"tweet_len\"]),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8600044-99bc-4960-8603-329a093d9ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.85      0.83       869\n",
      "           1       0.79      0.73      0.76       654\n",
      "\n",
      "    accuracy                           0.80      1523\n",
      "   macro avg       0.80      0.79      0.79      1523\n",
      "weighted avg       0.80      0.80      0.80      1523\n",
      "\n",
      "[[740 129]\n",
      " [179 475]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "lr_clf = Pipeline(\n",
    "    steps=[\n",
    "        (\"vectorizer\", text_features),\n",
    "        (\"model\", LogisticRegression(max_iter=2000))\n",
    "    ]\n",
    ")\n",
    "\n",
    "lr_clf.fit(X_train, y_train)\n",
    "y_pred_lr = lr_clf.predict(X_valid)\n",
    "\n",
    "print(classification_report(y_valid, y_pred_lr))\n",
    "print(confusion_matrix(y_valid, y_pred_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "81d9c99c-f8a8-4dbb-911e-208fe6baabcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "02e200ac-bd61-4313-802c-1956ff49dd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.82      0.82       869\n",
      "           1       0.76      0.76      0.76       654\n",
      "\n",
      "    accuracy                           0.79      1523\n",
      "   macro avg       0.79      0.79      0.79      1523\n",
      "weighted avg       0.79      0.79      0.79      1523\n",
      "\n",
      "[[715 154]\n",
      " [160 494]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "svm_clf = Pipeline(\n",
    "    steps=[\n",
    "        (\"vectorizer\", text_features),\n",
    "        (\"model\", LinearSVC(dual=\"auto\", max_iter=20000, tol=1e-3))\n",
    "    ]\n",
    ")\n",
    "\n",
    "svm_clf.fit(X_train, y_train)\n",
    "y_pred_svm = svm_clf.predict(X_valid)\n",
    "\n",
    "print(classification_report(y_valid, y_pred_svm))\n",
    "print(confusion_matrix(y_valid, y_pred_svm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "461f0c81-50ae-412d-96f0-7afe68f4c0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.95      0.83       869\n",
      "           1       0.89      0.55      0.68       654\n",
      "\n",
      "    accuracy                           0.78      1523\n",
      "   macro avg       0.81      0.75      0.75      1523\n",
      "weighted avg       0.80      0.78      0.76      1523\n",
      "\n",
      "[[825  44]\n",
      " [296 358]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "nb_clf = Pipeline(\n",
    "    steps=[\n",
    "        (\"vectorizer\", text_features),\n",
    "        (\"model\", MultinomialNB())\n",
    "    ]\n",
    ")\n",
    "\n",
    "nb_clf.fit(X_train, y_train)\n",
    "y_pred_nb = nb_clf.predict(X_valid)\n",
    "\n",
    "print(classification_report(y_valid, y_pred_nb))\n",
    "print(confusion_matrix(y_valid, y_pred_nb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dc6fd79c-be0d-4dd3-b4d3-91b0f02d0a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold scores: [0.75862069 0.7631786  0.73545384 0.74396135 0.74528302]\n",
      "Mean F1: 0.7492995001788447\n",
      "Std: 0.010157206443835578\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(svm_clf, X, y, cv=cv, scoring=\"f1\")\n",
    "\n",
    "print(\"Fold scores:\", scores)\n",
    "print(\"Mean F1:\", scores.mean())\n",
    "print(\"Std:\", scores.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6137a3e6-42cc-49d4-8bf4-66ab6533afdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   id  target\n",
       " 0   0       1\n",
       " 1   2       1\n",
       " 2   3       1\n",
       " 3   9       1\n",
       " 4  11       1,\n",
       " target\n",
       " 0    1978\n",
       " 1    1285\n",
       " Name: count, dtype: int64,\n",
       " (3263, 3263))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_full = train_df[[\"keyword\", \"location\", \"text\", \"tweet_len\"]]\n",
    "y_full = train_df[\"target\"]\n",
    "X_test = test_df[[\"keyword\", \"location\", \"text\", \"tweet_len\"]]\n",
    "\n",
    "svm_clf.fit(X_full, y_full)\n",
    "test_pred = svm_clf.predict(X_test)\n",
    "\n",
    "sub_df[\"target\"] = test_pred\n",
    "sub_df.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "sub_df.head(), sub_df[\"target\"].value_counts(), (len(test_pred), len(sub_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0313ec-770e-496d-9080-4023b91f7c88",
   "metadata": {},
   "source": [
    "# NLP Disaster Tweets Classification\n",
    "\n",
    "## Project overview\n",
    "This project uses NLP and machine learning to classify tweets as disaster-related (1) or not disaster-related (0) using the Kaggle “NLP Getting Started” dataset.\n",
    "\n",
    "## Dataset\n",
    "The dataset includes `train.csv`, `test.csv`, and `sample_submission.csv`.  \n",
    "Training data contains: `id`, `keyword`, `location`, `text`, `target`. Test data contains: `id`, `keyword`, `location`, `text`.\n",
    "\n",
    "## Data preprocessing\n",
    "- Filled missing values in `keyword` and `location` using `\"missing\"`.\n",
    "- Cleaned text by lowercasing, removing URLs/HTML tags/special characters, tokenizing, removing stopwords, and lemmatizing.\n",
    "\n",
    "## Feature extraction\n",
    "- Used TF-IDF with unigrams and bigrams (`ngram_range=(1,2)`) for `keyword`, `location`, and `text`.\n",
    "- Added an extra numeric feature: tweet length (`tweet_len`).\n",
    "\n",
    "## Model training\n",
    "- Split the data into train/validation with stratification.\n",
    "- Trained and compared multiple models: Logistic Regression, Multinomial Naive Bayes, and Linear SVM (LinearSVC).\n",
    "- Used scikit-learn Pipelines to combine feature extraction and modeling.\n",
    "\n",
    "## Evaluation results\n",
    "- Linear SVM (tuned) validation accuracy: 0.79\n",
    "- Class 0 F1-score: 0.82, Class 1 F1-score: 0.76\n",
    "- Confusion matrix: [[715, 154], [160, 494]]\n",
    "- 5-fold CV Mean F1: 0.7493, Std: 0.0102\n",
    "\n",
    "## Final model and submission\n",
    "The final model (LinearSVC) was trained on the full training set and used to predict the test set.  \n",
    "A `submission.csv` file was generated in the required Kaggle format (`id`, `target`), with 3263 predictions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
